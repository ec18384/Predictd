{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NOTICE\n",
    "The code below is modified for the purpose of building a classification API for this project. However, it should be fully credited that this code belongs to the author (Khalid Z, 2022) and can be found at: https://www.kaggle.com/code/zeyadkhalid/mbti-personality-types-classification-73-accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visual Explanation of the algorithm\n",
    "Choose one of the following:<br>\n",
    "[PowerPoint slide show (Recommended)](https://docs.google.com/presentation/d/1CNQgWNvMKb862dwAmO5c-pP5g3kMtSRJ/edit?usp=sharing&ouid=109931366324041339501&rtpof=true&sd=true)<br>\n",
    "[PDF](https://drive.google.com/file/d/1qXSAlIorIGyhkQM0oDFHeGn4ejUjNv8g/view?usp=sharing)<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "d65cc4b5",
   "metadata": {
    "papermill": {
     "duration": 0.01385,
     "end_time": "2021-12-31T13:06:19.116039",
     "exception": false,
     "start_time": "2021-12-31T13:06:19.102189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Important Update 1 (31-12-2021)\n",
    "**The dataset that was used to train the models, is now available at [MBTI 500 Dataset](https://www.kaggle.com/zeyadkhalid/mbti-personality-types-500-dataset)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdcce0",
   "metadata": {
    "papermill": {
     "duration": 0.013728,
     "end_time": "2021-12-31T13:06:19.143985",
     "exception": false,
     "start_time": "2021-12-31T13:06:19.130257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The algorithm classifies **Cognitive Functions**, and based on the top two cognitive functions we can tell which personality type it is\n",
    "\n",
    "Pre-trained models are **Logistic Regression binary classifiers**, trained on [MBTI 500 Dataset](https://www.kaggle.com/zeyadkhalid/mbti-personality-types-500-dataset), to classify (each) two cognitive functions. \n",
    "The **dataset** is significantly **imbalanced**, so I used only 6K samples for training, and 2K samples for validation (*validation data is uploaded with the pre-trained models*).\n",
    "\n",
    "**Note**: I’m sure that the pre-trained models have **high variance** (poor accuracy with different datasets), the top reason is that the data (texts) are all about MBTI, personality theories and how each type sees and copes with other personality types (because data were gathered from MBTI communities - they talk about a single subject). So you can test the algorithm on posts and comments from Reddit MBTI communities, you’ll see better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8df2934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:06:19.178543Z",
     "iopub.status.busy": "2021-12-31T13:06:19.176850Z",
     "iopub.status.idle": "2021-12-31T13:06:21.126723Z",
     "shell.execute_reply": "2021-12-31T13:06:21.125894Z",
     "shell.execute_reply.started": "2021-11-02T12:57:29.624836Z"
    },
    "papermill": {
     "duration": 1.968724,
     "end_time": "2021-12-31T13:06:21.126970",
     "exception": false,
     "start_time": "2021-12-31T13:06:19.158246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9966af31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:06:21.165766Z",
     "iopub.status.busy": "2021-12-31T13:06:21.164657Z",
     "iopub.status.idle": "2021-12-31T13:06:21.175045Z",
     "shell.execute_reply": "2021-12-31T13:06:21.175659Z",
     "shell.execute_reply.started": "2021-11-02T12:57:31.299611Z"
    },
    "papermill": {
     "duration": 0.03379,
     "end_time": "2021-12-31T13:06:21.175861",
     "exception": false,
     "start_time": "2021-12-31T13:06:21.142071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing tools\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "str_punc = string.punctuation\n",
    "\n",
    "engstopwords = stopwords.words(\"english\")\n",
    "engstopwordsV2 = re.sub('[' + re.escape(string.punctuation) + ']', '',\n",
    "                        ' '.join(engstopwords)).split()\n",
    "\n",
    "engstopwords = set(engstopwords).union(set(engstopwordsV2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b93feae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:06:21.212322Z",
     "iopub.status.busy": "2021-12-31T13:06:21.211408Z",
     "iopub.status.idle": "2021-12-31T13:06:21.228686Z",
     "shell.execute_reply": "2021-12-31T13:06:21.229455Z",
     "shell.execute_reply.started": "2021-11-02T12:57:32.794777Z"
    },
    "papermill": {
     "duration": 0.03771,
     "end_time": "2021-12-31T13:06:21.229666",
     "exception": false,
     "start_time": "2021-12-31T13:06:21.191956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to lemmatize a word using the three types: adjective, verb, noun\n",
    "def lemmatize_all_types(word):\n",
    "    word = wnl.lemmatize(word, 'a')\n",
    "    word = wnl.lemmatize(word, 'v')\n",
    "    word = wnl.lemmatize(word, 'n')\n",
    "    return word\n",
    "\n",
    "# Function to clean text\n",
    "def clean(text):\n",
    "    # Remove URLs from text\n",
    "    text = re.sub(\"http.*?([ ]|\\|\\|\\||$)\", \"\", text).lower()\n",
    "    url_regex = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    text = re.sub(url_regex, \"\", text)\n",
    "\n",
    "    # Remove specific punctuation (usually associated with a word)\n",
    "    text = re.sub(r'(:|;).', \" \", text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    text = re.sub('['+re.escape(str_punc)+']',\" \",  text)\n",
    "    \n",
    "    # Remove parantheses, brackets\n",
    "    text = re.sub('(\\[|\\()*\\d+(\\]|\\))*', ' ', text)\n",
    "    \n",
    "    # Remove string marks\n",
    "    text = re.sub('[’‘“\\.”…–]', '', text)\n",
    "    text = re.sub('[^(\\w|\\s)]', '', text)\n",
    "    text = re.sub('(gt|lt)', '', text)\n",
    "    \n",
    "    #Check that each word is not stopword, and lemmatize it\n",
    "    text = list(map(lemmatize_all_types, text.split()))\n",
    "    text = [word for word in text if (word not in engstopwords)]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "# Convert personality type into a dominant cognitive function\n",
    "def letters_to_functions(personality_type):\n",
    "    translator = {\n",
    "            'ENxP': 'Ne',\n",
    "            'INxJ': 'Ni',\n",
    "            'ESxP': 'Se',\n",
    "            'ISxJ': 'Si',\n",
    "            'ExTJ': 'Te',\n",
    "            'IxTP': 'Ti',\n",
    "            'IxFP': 'Fi',\n",
    "            'ExFJ': 'Fe',\n",
    "            \n",
    "            'xNxx': 'N',\n",
    "            'xSxx': 'S',\n",
    "            'xxTx' : 'T',\n",
    "            'xxFx': 'F',\n",
    "            'Ixxx':'I',\n",
    "            'Exxx':'E',\n",
    "            }\n",
    "    return translator[personality_type]\n",
    "\n",
    "# Convert a Cognitive Functions Stack into Personality Type\n",
    "def functions_to_letters(functions):\n",
    "    translator = {\n",
    "            'NiTe': 'INTJ',\n",
    "            'NiFe': 'INFJ',\n",
    "            'NeTi': 'ENTP',\n",
    "            'NeFi': 'ENFP',\n",
    "            'SiTe': 'ISTJ',\n",
    "            'SiFe': 'ISFJ',\n",
    "            'SeTi': 'ESTP',\n",
    "            'SeFi': 'ESFP',\n",
    "            'TeNi': 'ENTJ',\n",
    "            'FeNi': 'ENFJ',\n",
    "            'TiNe': 'INTP',\n",
    "            'FiNe': 'INFP',\n",
    "            'TeSi': 'ESTJ',\n",
    "            'FeSi': 'ESFJ',\n",
    "            'TiSe': 'ISTP',\n",
    "            'FiSe': 'ISFP',\n",
    "            }\n",
    "    return translator[functions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0856b0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:06:21.267944Z",
     "iopub.status.busy": "2021-12-31T13:06:21.266975Z",
     "iopub.status.idle": "2021-12-31T13:06:21.278585Z",
     "shell.execute_reply": "2021-12-31T13:06:21.279172Z",
     "shell.execute_reply.started": "2021-11-02T12:57:34.2003Z"
    },
    "papermill": {
     "duration": 0.034337,
     "end_time": "2021-12-31T13:06:21.279389",
     "exception": false,
     "start_time": "2021-12-31T13:06:21.245052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cognitive Functions meta-information\n",
    "cf_info = {\n",
    "        \"Ni\": {\n",
    "                'name':'intuition',\n",
    "                'role':'input',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        \"Ne\": {\n",
    "                'name':'intuition',\n",
    "                'role':'input',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        \"Si\": {\n",
    "                'name':'sensing',\n",
    "                'role':'input',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        \"Se\": {\n",
    "                'name':'sensing',\n",
    "                'role':'input',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        'Ti': {\n",
    "                'name':'thinking',\n",
    "                'role':'output',\n",
    "                'direction':'internal'\n",
    "                },\n",
    "        'Te': {\n",
    "                'name':'thinking',\n",
    "                'role':'output',\n",
    "                'direction':'external'\n",
    "                },\n",
    "        'Fi': {\n",
    "                'name':'feeling',\n",
    "                'role':'output',\n",
    "                'direction':'internal',\n",
    "                },\n",
    "        'Fe': {\n",
    "                'name':'feeling',\n",
    "                'role':'output',\n",
    "                'direction':'external'\n",
    "                }\n",
    "        }\n",
    "\n",
    "# Dictionary of pretrained models\n",
    "models = {\n",
    "        # Phase 1 models\n",
    "        'NiNe':None,\n",
    "        'NiSi':None,\n",
    "        'NiSe':None,\n",
    "        'NeSi':None,\n",
    "        'NeSe':None,\n",
    "        'SiSe':None,\n",
    "        \n",
    "        'TiTe':None,\n",
    "        'TiFi':None,\n",
    "        'TiFe':None,\n",
    "        'TeFi':None,\n",
    "        'TeFe':None,\n",
    "        'FiFe':None,\n",
    "            \n",
    "        # Phase 2 models\n",
    "    \n",
    "        # Scenario 1 (detect dominant cognitive function)\n",
    "        'NiTe':None,\n",
    "        'NiFe':None,\n",
    "        'SiTe':None,\n",
    "        'SiFe':None,\n",
    "        \n",
    "        'NeFi':None,\n",
    "        'NeTi':None,\n",
    "        'SeFi':None,\n",
    "        'SeTi':None,\n",
    "        \n",
    "        # Secenario 2 (fixing phase 1 result corruption)\n",
    "        'NiTi':None,\n",
    "        'NiFi':None,\n",
    "        'NeTe':None,\n",
    "        'NeFe':None,\n",
    "        \n",
    "        'SiTi':None,\n",
    "        'SiFi':None,\n",
    "        'SeTe':None,\n",
    "        'SeFe':None,\n",
    "        }\n",
    "\n",
    "# Supporter pretrained models, increases accuracy ~3%\n",
    "supporters = {\n",
    "        'NvS':None,\n",
    "        'FvT':None,\n",
    "        'IvE':None,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "217795a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:06:21.315595Z",
     "iopub.status.busy": "2021-12-31T13:06:21.314781Z",
     "iopub.status.idle": "2021-12-31T13:06:23.097792Z",
     "shell.execute_reply": "2021-12-31T13:06:23.097120Z",
     "shell.execute_reply.started": "2021-11-02T12:57:35.984316Z"
    },
    "papermill": {
     "duration": 1.80403,
     "end_time": "2021-12-31T13:06:23.097983",
     "exception": false,
     "start_time": "2021-12-31T13:06:21.293953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read models\n",
    "path = \"./mbti-pretrained-models/\"\n",
    "\n",
    "for key in models:\n",
    "    with open(path + key + '.pickle', 'rb') as file:\n",
    "        models[key] = pickle.load(file)\n",
    "\n",
    "for _key in supporters:\n",
    "    with open(path + _key + '.pickle', 'rb') as file:\n",
    "        supporters[_key] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14d64050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:06:23.416388Z",
     "iopub.status.busy": "2021-12-31T13:06:23.131794Z",
     "iopub.status.idle": "2021-12-31T13:06:23.440400Z",
     "shell.execute_reply": "2021-12-31T13:06:23.439634Z",
     "shell.execute_reply.started": "2021-11-02T12:57:37.512605Z"
    },
    "papermill": {
     "duration": 0.32795,
     "end_time": "2021-12-31T13:06:23.440590",
     "exception": false,
     "start_time": "2021-12-31T13:06:23.112640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the direction of a cognitive function (i => e | e => i)\n",
    "def flip_cf_direction(cognitiveFunction):\n",
    "    direction = cognitiveFunction[1]\n",
    "    new_direction = 'i' if direction == 'e' else 'e'\n",
    "    return cognitiveFunction[0] + new_direction\n",
    "\n",
    "# Pipeline for using a pre-trained model to predict sample\n",
    "def process_classify_sample(modelObject, sample):\n",
    "    vectorizer = modelObject['cv']\n",
    "    label_encoder = modelObject['labelEncoder']\n",
    "    model = modelObject['model']\n",
    "    \n",
    "    # Preprocessing\n",
    "    clean_sample = clean(sample)\n",
    "    x = vectorizer.transform([clean_sample]).toarray()\n",
    "    \n",
    "    # Classification\n",
    "    y = model.predict(x)\n",
    "    y_probability = max(model.predict_proba(x)[0])\n",
    "    classified_cf = label_encoder.inverse_transform(y)[0]\n",
    "    return letters_to_functions(classified_cf), y_probability\n",
    "\n",
    "    \n",
    "# Phase 1 of classifying a personality type\n",
    "# Recognize the top perceiving cognitive function\n",
    "# and the top judging cognitive function\n",
    "def phase1(sample):\n",
    "    # INPUT => sample: string\n",
    "    # OUTPUT => input_cf_acc, output_cf_acc: Series \n",
    "\n",
    "    # Keep track of every input (perceiving) cognitive function likelihood\n",
    "    input_cf_acc = pd.Series({\"Ni\":0, \"Ne\":0, \"Si\":0, \"Se\":0}, dtype=float)\n",
    "    # same for output (judging) cognitive functions\n",
    "    output_cf_acc = pd.Series({\"Ti\":0, \"Te\":0, \"Fi\":0, \"Fe\":0}, dtype=float)\n",
    "    \n",
    "    input_cf = np.array(['Ni', 'Ne', 'Si', 'Se'])\n",
    "    output_cf = np.array(['Ti', 'Te', 'Fi', 'Fe'])\n",
    "    \n",
    "    # nested loop for input (perceiving) cognitive functions\n",
    "    # so models are: NiNe, NiSi, NiSe,  ...\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 4):\n",
    "            model_name = input_cf[i] + input_cf[j]\n",
    "            modelObject = models[model_name]\n",
    "            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "            \n",
    "            # Incrase likelihood of the prediction class\n",
    "            input_cf_acc[cognitive_fn] += probability\n",
    "            other_cf = input_cf[i] if input_cf[j] == cognitive_fn else input_cf[j]\n",
    "            \n",
    "            # Increase likelihood (smaller value) of other classes\n",
    "            input_cf_acc[other_cf] += 1 - probability\n",
    "            \n",
    "    # Another nested loop for output (judging) cognitive functions\n",
    "    # so models are: TiTe, TiFi, TiFe, ...\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 4):\n",
    "            model_name = output_cf[i] + output_cf[j]\n",
    "            modelObject = models[model_name]\n",
    "            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "            \n",
    "            # Incrase likelihood of the prediction class\n",
    "            output_cf_acc[cognitive_fn] += probability\n",
    "            other_cf = output_cf[i] if output_cf[j] == cognitive_fn else output_cf[j]\n",
    "            \n",
    "            # Increase likelihood (smaller value) of other classes\n",
    "            output_cf_acc[other_cf] += 1 - probability\n",
    "    \n",
    "    # Use supporter model (iNtuition vs Sensing)\n",
    "    modelObject = supporters[\"NvS\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase iNtuitive functions\n",
    "    if cognitive_fn == \"N\":\n",
    "        input_cf_acc[['Ni', 'Ne']] += probability\n",
    "        input_cf_acc[['Si', 'Se']] += 1 - probability\n",
    "    # Increase Sensing functions\n",
    "    else:\n",
    "        input_cf_acc[['Si', 'Se']] += probability\n",
    "        input_cf_acc[['Ni', 'Ne']] += 1 - probability\n",
    "    \n",
    "    # Use supporter model (Feeling vs Thinking)\n",
    "    modelObject = supporters[\"FvT\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase Feeling functions likelihood\n",
    "    if cognitive_fn == \"F\":\n",
    "        output_cf_acc[['Fi', 'Fe']] += probability\n",
    "        output_cf_acc[['Ti', 'Te']] += 1 - probability\n",
    "    # Increase Thinking Functions likelihood\n",
    "    else:\n",
    "        output_cf_acc[['Ti', 'Te']] += probability\n",
    "        output_cf_acc[['Fi', 'Fe']] += 1 - probability   \n",
    "    \n",
    "    # Use supporter model (Introvert vs Extrovert)\n",
    "    modelObject = supporters[\"IvE\"]\n",
    "    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n",
    "    \n",
    "    # Increase Introverted functions likelihood\n",
    "    if cognitive_fn == \"I\":\n",
    "        input_cf_acc[['Ni', 'Si']] += probability\n",
    "        input_cf_acc[['Ne', 'Se']] += 1 - probability\n",
    "        \n",
    "        output_cf_acc[['Fi', 'Ti']] += probability\n",
    "        output_cf_acc[['Fe', 'Te']] += 1 - probability\n",
    "        \n",
    "    # Increase  Extroverted functions likelihood\n",
    "    else:\n",
    "        input_cf_acc[['Ne', 'Se']] += probability\n",
    "        input_cf_acc[['Ni', 'Si']] += 1 - probability\n",
    "        \n",
    "        output_cf_acc[['Fe', 'Te']] += probability\n",
    "        output_cf_acc[['Fi', 'Ti']] += 1 - probability\n",
    "    \n",
    "    # Return: likelihoods of perceiving (input) cognitive functions\n",
    "    # and judging (output) cogitive function\n",
    "    return input_cf_acc, output_cf_acc\n",
    "\n",
    "    \n",
    "    \n",
    "# Phase 2 of classification algorithm\n",
    "# Determine which cognitive function is the dominant\n",
    "# and which is the auxiliary.\n",
    "# and Fix the classification if necessary\n",
    "# Necessary: if phase 1 results a two cognitive functions\n",
    "# of the same direction (ex: Ni-Ti), this is not acceptable\n",
    "# since there's no personality with these Dom-Aux functions\n",
    "def phase2(sample, input_acc, output_acc):\n",
    "    # Number of classifications done on every cognitive functions\n",
    "    # (i.e. we ran 5 models having 'Ni' as one of it's classes)\n",
    "    counter_models_ran = 5\n",
    "    \n",
    "    # Get max-likelihood data (probability & className)\n",
    "    # maxInput is the perceiving function which got the maximum likelihood\n",
    "    # maxOutput is the judgning function ~~~~~~\n",
    "    maxInput = {'name':input_acc.idxmax(), 'proba':input_acc.max()}\n",
    "    maxOutput = {'name':output_acc.idxmax(), 'proba':output_acc.max()}\n",
    "    \n",
    "    # Get direction of each cognitive function\n",
    "    maxInputDirection = cf_info[maxInput['name']]['direction']\n",
    "    maxOutputDirection = cf_info[maxOutput['name']]['direction']\n",
    "    \n",
    "    # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n",
    "    cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n",
    "    phase2_model_name = \"\".join(cf_stack)\n",
    "    \n",
    "    # if both perceiving & judging classes (functions) are opposite direction\n",
    "    if maxInputDirection != maxOutputDirection:\n",
    "        # We know the top two cognitive functions (ie. Ni, Te)\n",
    "        # this path will run the appropriate models to\n",
    "        # determine which of these functions is Dominant (primary)\n",
    "        # and which is Auxiliary (secondary)\n",
    "\n",
    "        # determine which cognitive_function is the dominant one\n",
    "        modelObject = models[phase2_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "        \n",
    "    # both perceiving & judging functions have same direction (they must NOT)\n",
    "    else:\n",
    "        # There is an ambiguity, the top 2 cognitive_functions cannot be of\n",
    "        # the same direction (ie. Ni, Ti) (they're both 'i')\n",
    "        # One of them is correct, this function will detect which one is\n",
    "        \n",
    "        # Detect which of these functions is more accurate\n",
    "        modelObject = models[phase2_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "        \n",
    "        \n",
    "        # if dominant cognitive function is an input (perceiving) \n",
    "        # (ie: Ni, Ne, Si, Se)\n",
    "        # flip the direction of the other -output- function\n",
    "        # example: Ni-Ti  => Ni-Te\n",
    "        if dominant_cf_name == input_acc.idxmax():\n",
    "            problematic_cf = output_acc.idxmax()\n",
    "            input_acc[input_acc.idxmax()] += probability\n",
    "            fixed_cf_name = flip_cf_direction(problematic_cf)\n",
    "            temp_acc = output_acc[fixed_cf_name]\n",
    "            output_acc[fixed_cf_name] = output_acc[problematic_cf] + (1 - probability)\n",
    "            output_acc[problematic_cf] = temp_acc\n",
    "        \n",
    "        # else if the dominant cognitive function is an output (judging)\n",
    "        # (ie: Ti, Te, Fi, Fe)\n",
    "        # flip the direction of the other -input- function\n",
    "        # example: Ni-Ti => Ne-Ti\n",
    "        else:\n",
    "            problematic_cf = input_acc.idxmax()\n",
    "            output_acc[output_acc.idxmax()] += probability\n",
    "            fixed_cf_name = flip_cf_direction(problematic_cf)\n",
    "            temp_acc = input_acc[fixed_cf_name]\n",
    "            input_acc[fixed_cf_name] = input_acc[problematic_cf] + (1 - probability)\n",
    "            input_acc[problematic_cf] = temp_acc\n",
    "        \n",
    "        # Now we've fixed the direction of the (less) accurate cognitive function\n",
    "        # We need to determine which function is dominant\n",
    "        \n",
    "        # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n",
    "        cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n",
    "        phase2_final_model_name = \"\".join(cf_stack)\n",
    "\n",
    "        # Run model\n",
    "        modelObject = models[phase2_final_model_name]\n",
    "        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n",
    "        counter_models_ran += 1\n",
    "    \n",
    "    # Now we know which cognitive function is dominant and which is auxilary.\n",
    "    # handle their likelihoods\n",
    "    \n",
    "    # If dominant function is input (perceiving)\n",
    "    # increase its likelihood, decrease the other\n",
    "    if dominant_cf_name == input_acc.idxmax():\n",
    "        input_acc[input_acc.idxmax()] += probability\n",
    "        output_acc[output_acc.idxmax()] += 1 - probability\n",
    "\n",
    "    # else, dominant function is output (judging)\n",
    "    # increase its likelihood, decrease the other\n",
    "    else:\n",
    "        output_acc[output_acc.idxmax()] += probability\n",
    "        input_acc[input_acc.idxmax()] += 1 - probability\n",
    "\n",
    "    # Select the winners in phase 1\n",
    "    cognitive_functions_stack = pd.Series({\n",
    "            input_acc.idxmax(): input_acc[input_acc.idxmax()],\n",
    "            output_acc.idxmax(): output_acc[output_acc.idxmax()]\n",
    "            })\n",
    "    \n",
    "    # Select the dominant function as the 1st winner in phase 2\n",
    "    dominant_function =  dominant_cf_name\n",
    "    \n",
    "    # Select the auxiliary function as the 2nd winner in phase 2\n",
    "    auxiliary_function = cognitive_functions_stack.drop(dominant_cf_name).index[0]\n",
    "\n",
    "    # Convert cognitive functions to personality types\n",
    "    personality_type = functions_to_letters(dominant_function + auxiliary_function)\n",
    "    \n",
    "    # Calculate probability\n",
    "    probability = (cognitive_functions_stack[dominant_function] / counter_models_ran \n",
    "                   + cognitive_functions_stack[auxiliary_function] / counter_models_ran) / 2\n",
    "    \n",
    "    return personality_type, probability\n",
    "\n",
    "# Run classification algorithm phases\n",
    "def run(sample):\n",
    "    input_acc, output_acc = phase1(sample)\n",
    "    personality, probability = phase2(sample, input_acc, output_acc)\n",
    "    return personality, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6258132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:06:23.478240Z",
     "iopub.status.busy": "2021-12-31T13:06:23.477565Z",
     "iopub.status.idle": "2021-12-31T13:13:35.427176Z",
     "shell.execute_reply": "2021-12-31T13:13:35.426224Z",
     "shell.execute_reply.started": "2021-10-27T18:50:11.535247Z"
    },
    "papermill": {
     "duration": 431.972643,
     "end_time": "2021-12-31T13:13:35.427412",
     "exception": false,
     "start_time": "2021-12-31T13:06:23.454769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VALIDATION\n",
    "validation_set = pd.read_csv(path + 'validation_set.csv')\n",
    "\n",
    "def classify(sentence):\n",
    "    personality, probability = run(sentence)\n",
    "    return personality\n",
    "\n",
    "# Results as 16 personality types\n",
    "y_pred = validation_set['posts'].apply(classify) # it takes ~5 minutes to finish\n",
    "y_true = validation_set['type']\n",
    "\n",
    "# Results as 4 categories (NT, NF, ST, SF)\n",
    "y_pred_soft = y_pred.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')\n",
    "y_true_soft = y_true.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3c36534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:13:35.472692Z",
     "iopub.status.busy": "2021-12-31T13:13:35.471732Z",
     "iopub.status.idle": "2021-12-31T13:13:35.732133Z",
     "shell.execute_reply": "2021-12-31T13:13:35.731261Z",
     "shell.execute_reply.started": "2021-10-27T18:57:27.512691Z"
    },
    "papermill": {
     "duration": 0.284351,
     "end_time": "2021-12-31T13:13:35.732304",
     "exception": false,
     "start_time": "2021-12-31T13:13:35.447953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Classes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NF       0.84      0.83      0.84       649\n",
      "          NT       0.93      0.86      0.89      1202\n",
      "          SF       0.29      0.66      0.40        32\n",
      "          ST       0.54      0.77      0.63       117\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.65      0.78      0.69      2000\n",
      "weighted avg       0.87      0.84      0.85      2000\n",
      "\n",
      "\n",
      "16 Classes\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ENFJ       0.00      0.00      0.00         0\n",
      "        ENFP       0.54      0.70      0.61        74\n",
      "        ENTJ       0.00      0.00      0.00         0\n",
      "        ENTP       0.64      0.66      0.65       137\n",
      "        ESFJ       0.00      0.00      0.00         0\n",
      "        ESFP       0.25      0.33      0.29         3\n",
      "        ESTJ       0.00      0.00      0.00         0\n",
      "        ESTP       0.58      0.97      0.73        29\n",
      "        INFJ       0.79      0.67      0.72       310\n",
      "        INFP       0.78      0.75      0.76       265\n",
      "        INTJ       0.84      0.71      0.77       502\n",
      "        INTP       0.86      0.82      0.84       563\n",
      "        ISFJ       0.25      0.67      0.36        12\n",
      "        ISFP       0.20      0.35      0.26        17\n",
      "        ISTJ       0.27      0.65      0.38        17\n",
      "        ISTP       0.49      0.54      0.51        71\n",
      "\n",
      "    accuracy                           0.73      2000\n",
      "   macro avg       0.41      0.49      0.43      2000\n",
      "weighted avg       0.77      0.73      0.74      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports\n",
    "print(\"4 Classes\")\n",
    "print(classification_report(y_true_soft, y_pred_soft))\n",
    "\n",
    "print(\"\\n16 Classes\\n\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ec341a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:13:35.768633Z",
     "iopub.status.busy": "2021-12-31T13:13:35.767579Z",
     "iopub.status.idle": "2021-12-31T13:13:35.822882Z",
     "shell.execute_reply": "2021-12-31T13:13:35.822181Z",
     "shell.execute_reply.started": "2021-11-02T12:57:50.660428Z"
    },
    "papermill": {
     "duration": 0.07478,
     "end_time": "2021-12-31T13:13:35.823046",
     "exception": false,
     "start_time": "2021-12-31T13:13:35.748266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ENTJ\n",
      "Likelihood: 0.6925291958448305\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey, they call me the Commander, I create plans and strategies for everything,\n",
    "I solve problems using highly optimized solutions, and I use my intuition\n",
    "to predict possible scenarios in the future, some people consider me as bossy,\n",
    "but I'm not, recognized me?\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ENTJ\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "273719e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:13:35.862061Z",
     "iopub.status.busy": "2021-12-31T13:13:35.861001Z",
     "iopub.status.idle": "2021-12-31T13:13:35.919346Z",
     "shell.execute_reply": "2021-12-31T13:13:35.920131Z",
     "shell.execute_reply.started": "2021-11-02T12:58:13.114729Z"
    },
    "papermill": {
     "duration": 0.082215,
     "end_time": "2021-12-31T13:13:35.920412",
     "exception": false,
     "start_time": "2021-12-31T13:13:35.838197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ISFP\n",
      "Likelihood: 0.6744017846686068\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey, they call me the Adventurer, I do artworks and sometimes play guitar,\n",
    "I enjoy spending time alone listening to music, I appreciate authenticity\n",
    "and honesty, I'm always connected to my feelings and alert of it,\n",
    "some people consider as an artist, but I'm not, and I won't let them\n",
    "define what I am, regonized me?\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ISFP\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06ad0aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:13:35.973887Z",
     "iopub.status.busy": "2021-12-31T13:13:35.972944Z",
     "iopub.status.idle": "2021-12-31T13:13:36.017618Z",
     "shell.execute_reply": "2021-12-31T13:13:36.018360Z",
     "shell.execute_reply.started": "2021-10-27T18:57:39.286373Z"
    },
    "papermill": {
     "duration": 0.075618,
     "end_time": "2021-12-31T13:13:36.018604",
     "exception": false,
     "start_time": "2021-12-31T13:13:35.942986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ESTP\n",
      "Likelihood: 0.7212842011608569\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey guys, wanna hang out ? come on ..\n",
    "hiking out, flirting with girls, going to parties ..\n",
    "what's life without fun!\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # ESxP\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aed4d2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-31T13:13:36.067697Z",
     "iopub.status.busy": "2021-12-31T13:13:36.066789Z",
     "iopub.status.idle": "2021-12-31T13:13:36.132817Z",
     "shell.execute_reply": "2021-12-31T13:13:36.133352Z",
     "shell.execute_reply.started": "2021-10-27T18:57:41.495441Z"
    },
    "papermill": {
     "duration": 0.092572,
     "end_time": "2021-12-31T13:13:36.133585",
     "exception": false,
     "start_time": "2021-12-31T13:13:36.041013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: INTP\n",
      "Likelihood: 0.6571975312658572\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    "Hey. .. .. .. what? .. .. ..\n",
    "okay I forgot. Hi, I'm the Logician, or at least that's what they call me besides 'the robot',\n",
    "I use my critical thinking skills to logically analyze everything,\n",
    "I spend a lot of my time thinking about solutions for problems that will never occur,\n",
    "reading articles, investigating theories, understanding concepts, it's all about my mind.\n",
    "I know I'm in a computer program being tested, I recognized you ..\n",
    "\"\"\"\n",
    "personality, probability = run(sentence) # INTP\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality: ISTP\n",
      "Likelihood: 0.6182640535119441\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"    Hey. .. .. .. what? .. .. ..\n",
    "    okay I forgot. Hi, I'm the Logician, or at least that's what they call me besides 'the robot',\n",
    "    I use my e it's all about my mind.\n",
    "    I know I'm in a computer program being tested, I recognized you ..\"\"\"\n",
    "personality, probability = run(sentence) #\n",
    "print(f\"Personality: {personality}\\nLikelihood: {probability}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import anvil.server\n",
    "anvil.server.connect(\"N2D2D6ERWTFT3BOVYCCY7DNR-25RBPVHG52GKMPG7\")\n",
    "\n",
    "\n",
    "@anvil.server.http_endpoint(\"/prediction\")\n",
    "def predict(**params):\n",
    "\n",
    "    ip = anvil.server.request.remote_address\n",
    "\n",
    "    query_params_json = json.dumps(params)\n",
    "    query_params_json_obj = json.loads(query_params_json)\n",
    "\n",
    "\n",
    "    input = query_params_json_obj[\"input\"]\n",
    "    target = query_params_json_obj[\"target\"]\n",
    "    profilePicUrl = query_params_json_obj[\"profilePicUrl\"]\n",
    "\n",
    "    # Run prediction\n",
    "    personality, probability = run(input)\n",
    "\n",
    "\n",
    "    if personality[0] == \"I\":\n",
    "        IvsE = \"I\"\n",
    "    else:\n",
    "        IvsE = \"E\"\n",
    "\n",
    "    if personality[1] == \"N\":\n",
    "        IvsS = \"N\"\n",
    "    else:\n",
    "        IvsS = \"S\"\n",
    "\n",
    "    if personality[2] == \"T\":\n",
    "        TvsF = \"T\"\n",
    "    else:\n",
    "        TvsF = \"F\"\n",
    "\n",
    "    if personality[3] == \"J\":\n",
    "        JvsP = \"J\"\n",
    "    else:\n",
    "        JvsP = \"P\"\n",
    "\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "    probabilityAsPercentage = probability * 100\n",
    "    probability = \"{:.2f}\".format(probabilityAsPercentage)\n",
    "\n",
    "    return {\"type\": personality,\n",
    "            \"IvsE\": IvsE,\n",
    "            \"IvsS\": IvsS,\n",
    "            \"TvsF\": TvsF,\n",
    "            \"JvsP\": JvsP,\n",
    "            \"probability\": probability,\n",
    "            \"IP Address\": ip,\n",
    "            \"target\": target,\n",
    "            \"date\": date,\n",
    "            \"profilePicUrl\": profilePicUrl}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 451.462032,
   "end_time": "2021-12-31T13:13:37.767778",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-12-31T13:06:06.305746",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}